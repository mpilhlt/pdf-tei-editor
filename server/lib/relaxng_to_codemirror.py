#!/usr/bin/env python3
"""
RelaxNG to CodeMirror Autocomplete Map Converter

This module converts a RelaxNG schema file to a JSON autocomplete map
suitable for use with CodeMirror's XML mode.

Code generated by Claude Code with instructions by @cboulanger

Usage:
    from relaxng_to_codemirror import generate_autocomplete_map
    
    # Generate map from RelaxNG file
    autocomplete_map = generate_autocomplete_map('schema.rng')
    
    # Save to JSON file
    import json
    with open('autocomplete.json', 'w') as f:
        json.dump(autocomplete_map, f, indent=2)
"""

import xml.etree.ElementTree as ET
from typing import Dict, List, Set, Optional, Union
from collections import defaultdict
import json


class RelaxNGParser:
    """
    Parser for RelaxNG schema files.
    
    When deduplicate=True, the output includes a JavaScript helper function
    to resolve deduplicated references:
    
    ```javascript
    function resolveDeduplicated(data) {
        // Create a copy to avoid modifying the original
        const resolved = JSON.parse(JSON.stringify(data));
        
        // Extract reference definitions (keys starting with #)
        const refs = {};
        Object.keys(resolved).forEach(key => {
            if (key.startsWith('#')) {
                refs[key] = resolved[key];
                delete resolved[key];
            }
        });
        
        // Recursive function to resolve references (including composite ones and macros)
        function resolveRefs(obj) {
            if (typeof obj === 'string' && obj.includes('#')) {
                if (obj.startsWith('#M')) {
                    // Macro reference - resolve the macro first, then resolve its contents
                    const macroContent = refs[obj];
                    if (macroContent && typeof macroContent === 'string') {
                        return resolveRefs(macroContent);
                    }
                    return obj;
                } else if (obj.startsWith('#') && !obj.includes(' ')) {
                    // Simple reference
                    return refs[obj] ? JSON.parse(JSON.stringify(refs[obj])) : obj;
                } else if (obj.includes(' ')) {
                    // Composite reference like "#1 #23 #44"
                    const refIds = obj.split(' ').filter(id => id.startsWith('#'));
                    return mergeReferences(refIds, refs);
                }
                return obj;
            } else if (Array.isArray(obj)) {
                return obj.map(resolveRefs);
            } else if (obj && typeof obj === 'object') {
                const result = {};
                Object.keys(obj).forEach(key => {
                    result[key] = resolveRefs(obj[key]);
                });
                return result;
            }
            return obj;
        }
        
        // Function to merge multiple references into a single object/array
        function mergeReferences(refIds, refs) {
            const resolved = refIds.map(id => refs[id]).filter(Boolean);
            
            if (resolved.length === 0) return null;
            if (resolved.length === 1) return JSON.parse(JSON.stringify(resolved[0]));
            
            // Determine merge strategy based on types
            const firstType = Array.isArray(resolved[0]) ? 'array' : typeof resolved[0];
            
            if (firstType === 'object' && resolved.every(r => typeof r === 'object' && !Array.isArray(r))) {
                // Merge objects
                const merged = {};
                resolved.forEach(obj => Object.assign(merged, obj));
                return merged;
            } else if (firstType === 'array' && resolved.every(r => Array.isArray(r))) {
                // Concatenate arrays
                return [].concat(...resolved);
            } else {
                // Mixed types - return as array
                return resolved;
            }
        }
        
        // Resolve all references in the main data
        Object.keys(resolved).forEach(key => {
            resolved[key] = resolveRefs(resolved[key]);
        });
        
        return resolved;
    }
    
    // Usage:
    // const autocompleteMap = resolveDeduplicated(rawData);
    ```
    """
    
    def __init__(self, include_global_attrs: bool = True, sort_alphabetically: bool = True, deduplicate: bool = False, debug_dedup: bool = False):
        self.include_global_attrs = include_global_attrs
        self.sort_alphabetically = sort_alphabetically
        self.deduplicate = deduplicate
        self.debug_dedup = debug_dedup
        self.namespace_map = {}
        self.defined_patterns = {}
        self.element_definitions = {}
        self.attribute_definitions = defaultdict(dict)
        self.processing_stack = set()  # Track currently processing patterns to avoid cycles
        
        # Enhanced deduplication tracking
        self.data_signatures = {}  # signature -> reference_id
        self.reference_data = {}   # reference_id -> actual_data
        self.next_ref_id = 1
        
        # Compositional deduplication
        self.atomic_parts = {}     # signature -> ref_id for atomic parts
        self.composite_cache = {}  # frozenset of ref_ids -> composite_ref_id
        self.part_usage = defaultdict(int)  # Track usage frequency
        
        # Macro optimization for common composite patterns
        self.composite_patterns = defaultdict(int)  # composite_string -> usage_count
        self.macro_refs = {}       # composite_string -> macro_ref_id
        
        # Debug stats
        self.dedup_stats = {
            'simple_dedup': 0,
            'compositional_dedup': 0,
            'atomic_parts_created': 0,
            'compositions_created': 0,
            'compositions_rejected': 0,
            'macros_created': 0,
            'macro_usages': 0
        }
        
    def parse_file(self, file_path: str) -> Dict[str, Dict]:
        """Parse a RelaxNG file and return autocomplete map."""
        try:
            tree = ET.parse(file_path)
            root = tree.getroot()
            
            # Extract namespace prefixes
            self._extract_namespaces(root)
            
            # First pass: collect all define patterns
            self._collect_define_patterns(root)
            
            # Second pass: process element definitions
            self._process_elements(root)
            
            # Build the final autocomplete map
            return self._build_autocomplete_map()
            
        except ET.ParseError as e:
            raise ValueError(f"Failed to parse RelaxNG file: {e}")
        except FileNotFoundError:
            raise FileNotFoundError(f"RelaxNG file not found: {file_path}")
    
    def _extract_namespaces(self, root: ET.Element) -> None:
        """Extract namespace prefixes from the root element."""
        for key, value in root.attrib.items():
            if key.startswith('xmlns:'):
                prefix = key[6:]  # Remove 'xmlns:' prefix
                self.namespace_map[prefix] = value
    
    def _collect_define_patterns(self, element: ET.Element) -> None:
        """Collect all define patterns for later reference resolution."""
        for define in element.findall('.//{http://relaxng.org/ns/structure/1.0}define'):
            name = define.get('name')
            if name:
                self.defined_patterns[name] = define
        
        # Also collect from included grammars
        for include in element.findall('.//{http://relaxng.org/ns/structure/1.0}include'):
            # In a real implementation, you'd parse included files
            pass
    
    def _process_elements(self, root: ET.Element) -> None:
        """Process all element definitions in the schema."""
        # Find all element definitions
        for element in root.findall('.//{http://relaxng.org/ns/structure/1.0}element'):
            name = element.get('name')
            if name:
                self._process_element_definition(name, element)
    
    def _process_element_definition(self, name: str, element: ET.Element) -> None:
        """Process a single element definition."""
        # Extract child elements
        children = self._extract_child_elements(element)
        
        # Extract attributes
        attributes = self._extract_attributes(element)
        
        # Extract documentation
        documentation = self._extract_documentation(element)
        
        element_data = {
            'children': children,
            'attrs': attributes
        }
        
        if documentation:
            element_data['doc'] = documentation
            
        self.element_definitions[name] = element_data
    
    def _extract_child_elements(self, element: ET.Element, visited: Optional[Set[str]] = None) -> List[str]:
        """Extract allowed child elements from an element definition."""
        if visited is None:
            visited = set()
            
        children = set()
        
        # Look for direct child elements (only immediate children, not descendants)
        for child_elem in element.findall('./{http://relaxng.org/ns/structure/1.0}element'):
            child_name = child_elem.get('name')
            if child_name:
                children.add(child_name)
        
        # Look for child elements in choice/group/optional/zeroOrMore/oneOrMore/interleave
        for container in element.findall('./{http://relaxng.org/ns/structure/1.0}choice') + \
                         element.findall('./{http://relaxng.org/ns/structure/1.0}group') + \
                         element.findall('./{http://relaxng.org/ns/structure/1.0}optional') + \
                         element.findall('./{http://relaxng.org/ns/structure/1.0}zeroOrMore') + \
                         element.findall('./{http://relaxng.org/ns/structure/1.0}oneOrMore') + \
                         element.findall('./{http://relaxng.org/ns/structure/1.0}interleave'):
            container_children = self._extract_child_elements(container, visited)
            children.update(container_children)
        
        # Look for references to other patterns (with cycle detection)
        for ref in element.findall('./{http://relaxng.org/ns/structure/1.0}ref'):
            ref_name = ref.get('name')
            if ref_name and ref_name in self.defined_patterns and ref_name not in visited:
                visited.add(ref_name)
                try:
                    ref_children = self._extract_child_elements(self.defined_patterns[ref_name], visited)
                    children.update(ref_children)
                finally:
                    visited.discard(ref_name)
        
        result = list(children)
        if self.sort_alphabetically:
            result.sort()
                
            # Show created macros
            if self.macro_refs:
                print(f"\nCreated macros:")
                for pattern, macro_ref in self.macro_refs.items():
                    count = self.composite_patterns[pattern]
                    print(f"    {macro_ref}: {count} uses - {pattern}")
            elif self.composite_patterns:
                print(f"\nNo macros created (all patterns below threshold)")
        
        return result
    
    def _extract_documentation(self, element: ET.Element) -> Optional[str]:
        """Extract documentation from RelaxNG schema element."""
        docs = []
        
        # Look for immediate a:documentation elements (avoid deep nesting)
        for doc in element.findall('./a:documentation', {'a': 'http://relaxng.org/ns/compatibility/annotations/1.0'}):
            if doc.text and doc.text.strip():
                docs.append(doc.text.strip())
        
        # Return combined documentation or None
        return ' '.join(docs) if docs else None
    
    def _extract_attributes(self, element: ET.Element, visited: Optional[Set[str]] = None) -> Dict[str, Union[List[str], Dict, None]]:
        """Extract attribute definitions from an element."""
        if visited is None:
            visited = set()
            
        attributes = {}
        
        # Find all attribute definitions (only direct children, not descendants)
        for attr in element.findall('./{http://relaxng.org/ns/structure/1.0}attribute'):
            attr_name = attr.get('name')
            if attr_name:
                # Check for value constraints
                values = self._extract_attribute_values(attr)
                # Extract documentation for this attribute
                attr_doc = self._extract_documentation(attr)
                
                # Create attribute data structure
                if attr_doc and values:
                    attributes[attr_name] = {'values': values, 'doc': attr_doc}
                elif attr_doc:
                    attributes[attr_name] = {'doc': attr_doc}
                else:
                    attributes[attr_name] = values
        
        # Look for attributes in containers
        for container in element.findall('./{http://relaxng.org/ns/structure/1.0}choice') + \
                         element.findall('./{http://relaxng.org/ns/structure/1.0}group') + \
                         element.findall('./{http://relaxng.org/ns/structure/1.0}optional') + \
                         element.findall('./{http://relaxng.org/ns/structure/1.0}zeroOrMore') + \
                         element.findall('./{http://relaxng.org/ns/structure/1.0}oneOrMore') + \
                         element.findall('./{http://relaxng.org/ns/structure/1.0}interleave'):
            container_attrs = self._extract_attributes(container, visited)
            # Merge attributes, preserving documentation
            for attr_name, attr_data in container_attrs.items():
                if attr_name not in attributes:
                    attributes[attr_name] = attr_data
                elif isinstance(attributes[attr_name], dict) and isinstance(attr_data, dict):
                    # Merge both dict structures
                    merged = attributes[attr_name].copy()
                    merged.update(attr_data)
                    attributes[attr_name] = merged
        
        # Look for attribute references (with cycle detection)
        for ref in element.findall('./{http://relaxng.org/ns/structure/1.0}ref'):
            ref_name = ref.get('name')
            if ref_name and ref_name in self.defined_patterns and ref_name not in visited:
                visited.add(ref_name)
                try:
                    ref_attrs = self._extract_attributes(self.defined_patterns[ref_name], visited)
                    # Merge referenced attributes, preserving documentation
                    for attr_name, attr_data in ref_attrs.items():
                        if attr_name not in attributes:
                            attributes[attr_name] = attr_data
                        elif isinstance(attributes[attr_name], dict) and isinstance(attr_data, dict):
                            # Merge both dict structures
                            merged = attributes[attr_name].copy()
                            merged.update(attr_data)
                            attributes[attr_name] = merged
                finally:
                    visited.discard(ref_name)
        
        return attributes
    
    def _extract_attribute_values(self, attr_element: ET.Element) -> Union[List[str], None]:
        """Extract possible values for an attribute."""
        values = set()
        
        # Look for choice elements with value constraints
        for choice in attr_element.findall('.//{http://relaxng.org/ns/structure/1.0}choice'):
            for value in choice.findall('.//{http://relaxng.org/ns/structure/1.0}value'):
                if value.text:
                    values.add(value.text.strip())
        
        # Look for direct value elements
        for value in attr_element.findall('.//{http://relaxng.org/ns/structure/1.0}value'):
            if value.text:
                values.add(value.text.strip())
        
        # Check for data type constraints
        for data in attr_element.findall('.//{http://relaxng.org/ns/structure/1.0}data'):
            data_type = data.get('type')
            if data_type == 'boolean':
                values.update(['true', 'false'])
        
        # Return None for open attributes, sorted list for constrained ones
        return sorted(list(values)) if values else None
    
    def _build_autocomplete_map(self) -> Dict[str, Dict]:
        """Build the final autocomplete map."""
        result = {}
        
        for element_name, definition in self.element_definitions.items():
            element_data = {}
            
            # Add children if any exist
            if definition['children']:
                children = definition['children']
                if self.deduplicate:
                    children = self._maybe_deduplicate(children)
                element_data['children'] = children
            
            # Add attributes
            attrs = definition['attrs'].copy()
            
            # Add global attributes if requested
            if self.include_global_attrs:
                global_attrs = self._get_global_attributes()
                # Don't override existing attributes
                for attr_name, attr_values in global_attrs.items():
                    if attr_name not in attrs:
                        attrs[attr_name] = attr_values
            
            if attrs:
                if self.deduplicate:
                    attrs = self._maybe_deduplicate(attrs)
                element_data['attrs'] = attrs
            
            # Add documentation if available
            if 'doc' in definition and definition['doc']:
                element_data['doc'] = definition['doc']
            
            # Only include element if it has children, attributes, or documentation
            if element_data:
                if self.deduplicate:
                    element_data = self._maybe_deduplicate(element_data)
                result[element_name] = element_data
        
        # Add reference data if deduplicating
        if self.deduplicate:
            # First pass: collect composite pattern usage
            self._collect_composite_patterns(result)
            
            # Second pass: create macros for frequent patterns
            self._create_macros()
            
            # Third pass: replace composite patterns with macros
            self._apply_macros(result)
            
            result.update(self.reference_data)
            
            # Print debug stats
            if self.debug_dedup:
                print(f"\nDeduplication Stats:")
                print(f"  Simple deduplications: {self.dedup_stats['simple_dedup']}")
                print(f"  Compositional deduplications: {self.dedup_stats['compositional_dedup']}")
                print(f"  Atomic parts created: {self.dedup_stats['atomic_parts_created']}")
                print(f"  Compositions created: {self.dedup_stats['compositions_created']}")
                print(f"  Compositions rejected: {self.dedup_stats['compositions_rejected']}")
                print(f"  Macros created: {self.dedup_stats['macros_created']}")
                print(f"  Macro usages: {self.dedup_stats['macro_usages']}")
                print(f"  Total references: {len(self.reference_data)}")
                
                # Show most used atomic parts
                if self.part_usage:
                    print(f"\nMost used atomic parts:")
                    for ref_id, count in sorted(self.part_usage.items(), key=lambda x: x[1], reverse=True)[:10]:
                        if ref_id in self.reference_data:
                            data_preview = json.dumps(self.reference_data[ref_id])[:60]
                            print(f"    {ref_id}: {count} uses - {data_preview}...")
                
                # Show composite patterns found
                if self.composite_patterns:
                    print(f"\nComposite patterns found:")
                    for pattern, count in sorted(self.composite_patterns.items(), key=lambda x: x[1], reverse=True)[:10]:
                        print(f"    '{pattern}': {count} uses")
        
        return result
    
    def _maybe_deduplicate(self, data) -> Union[str, Dict, List]:
        """
        Enhanced deduplication with compositional references.
        Can create references like "#1 #23 #44" for composite structures.
        """
        if not self.deduplicate:
            return data
        
        # Only deduplicate complex structures
        if isinstance(data, dict) and len(data) <= 1:
            return data
        if isinstance(data, list) and len(data) <= 1:
            return data
        if not isinstance(data, (dict, list)):
            return data
        
        # Try compositional decomposition first
        composite_ref = self._try_compositional_dedup(data)
        if composite_ref:
            self.dedup_stats['compositional_dedup'] += 1
            if self.debug_dedup:
                print(f"COMPOSITIONAL: {json.dumps(data)[:100]}... -> {composite_ref}")
            return composite_ref
        
        # Fall back to simple deduplication
        signature = self._create_signature(data)
        
        if signature in self.data_signatures:
            self.dedup_stats['simple_dedup'] += 1
            return self.data_signatures[signature]
        
        # This is new data, create a reference for it
        ref_id = f"#{self.next_ref_id}"
        self.next_ref_id += 1
        
        self.data_signatures[signature] = ref_id
        self.reference_data[ref_id] = data
        self.dedup_stats['simple_dedup'] += 1
        
        if self.debug_dedup:
            print(f"SIMPLE: {json.dumps(data)[:100]}... -> {ref_id}")
        
        return ref_id
    
    def _try_compositional_dedup(self, data) -> Optional[str]:
        """
        Try to decompose data into atomic parts and create compositional references.
        Returns a composite reference like "#1 #23 #44" or None if not beneficial.
        """
        if isinstance(data, dict):
            return self._decompose_dict(data)
        elif isinstance(data, list):
            return self._decompose_list(data)
        return None
    
    def _decompose_dict(self, data: dict) -> Optional[str]:
        """Decompose a dictionary into atomic parts."""
        if len(data) < 2:
            return None
        
        # Find atomic parts (individual key-value pairs or small groups)
        atomic_refs = []
        remaining_items = list(data.items())
        
        # First pass: find existing atomic parts
        for key, value in remaining_items[:]:
            atomic_data = {key: value}
            atomic_sig = self._create_signature(atomic_data)
            
            if atomic_sig in self.atomic_parts:
                atomic_refs.append(self.atomic_parts[atomic_sig])
                remaining_items.remove((key, value))
                self.part_usage[self.atomic_parts[atomic_sig]] += 1
        
        # Second pass: create new atomic parts for frequently occurring patterns
        for key, value in remaining_items[:]:
            atomic_data = {key: value}
            atomic_sig = self._create_signature(atomic_data)
            
            # Check if this pattern might be worth atomizing
            if self._should_atomize(atomic_data):
                ref_id = f"#{self.next_ref_id}"
                self.next_ref_id += 1
                self.atomic_parts[atomic_sig] = ref_id
                self.reference_data[ref_id] = atomic_data
                atomic_refs.append(ref_id)
                remaining_items.remove((key, value))
                self.part_usage[ref_id] += 1
                self.dedup_stats['atomic_parts_created'] += 1
        
        # If we have remaining items, group them
        if remaining_items:
            remaining_dict = dict(remaining_items)
            remaining_sig = self._create_signature(remaining_dict)
            
            if remaining_sig in self.data_signatures:
                atomic_refs.append(self.data_signatures[remaining_sig])
            else:
                ref_id = f"#{self.next_ref_id}"
                self.next_ref_id += 1
                self.data_signatures[remaining_sig] = ref_id
                self.reference_data[ref_id] = remaining_dict
                atomic_refs.append(ref_id)
        
        # Only return composite if we have multiple parts and it's beneficial
        if len(atomic_refs) >= 2 and self._is_composition_beneficial(atomic_refs, data):
            self.dedup_stats['compositions_created'] += 1
            if self.debug_dedup:
                print(f"COMPOSITION CREATED: {atomic_refs} for {json.dumps(data)[:60]}...")
            return " ".join(sorted(atomic_refs))  # Sort for consistency
        elif len(atomic_refs) >= 2:
            self.dedup_stats['compositions_rejected'] += 1
            if self.debug_dedup:
                print(f"COMPOSITION REJECTED: {atomic_refs} for {json.dumps(data)[:60]}...")
        
        return None
    
    def _decompose_list(self, data: list) -> Optional[str]:
        """Decompose a list into atomic parts."""
        if len(data) < 4:  # Only decompose longer lists
            return None
        
        # Try to find common sublists
        atomic_refs = []
        remaining_items = data[:]
        
        # Look for sublists that appear elsewhere
        for i in range(0, len(data), 2):  # Try chunks of 2
            if i + 1 < len(data):
                chunk = data[i:i+2]
                chunk_sig = self._create_signature(chunk)
                
                if chunk_sig in self.atomic_parts:
                    atomic_refs.append(self.atomic_parts[chunk_sig])
                    # Remove from remaining (this is simplified - real implementation would be more careful)
                elif self._should_atomize(chunk):
                    ref_id = f"#{self.next_ref_id}"
                    self.next_ref_id += 1
                    self.atomic_parts[chunk_sig] = ref_id
                    self.reference_data[ref_id] = chunk
                    atomic_refs.append(ref_id)
        
        # For lists, compositional decomposition is less beneficial
        # so we're conservative here
        return None
    
    def _should_atomize(self, data) -> bool:
        """
        Determine if a piece of data should be turned into an atomic reference.
        Uses heuristics based on size and potential reusability.
        """
        if isinstance(data, dict):
            # Atomize single key-value pairs that look reusable
            if len(data) == 1:
                key, value = next(iter(data.items()))
                # Common TEI attributes are good candidates
                return key in ['xml:lang', 'xml:id', 'n', 'rend', 'type', 'cert', 'resp']
            # Atomize small dicts with common patterns
            return len(data) <= 3 and any(k in data for k in ['xml:lang', 'xml:id', 'n', 'rend'])
        elif isinstance(data, list):
            # Atomize short lists of common elements
            return len(data) <= 3 and all(isinstance(item, str) for item in data)
        
        return False
    
    def _is_composition_beneficial(self, atomic_refs: List[str], original_data) -> bool:
        """
        Determine if creating a composition is more beneficial than storing as-is.
        """
        # Calculate space of composition vs original
        composition_space = len(" ".join(atomic_refs))
        original_space = len(json.dumps(original_data, separators=(',', ':')))
        
        # Only beneficial if composition is significantly smaller
        # and we expect the parts to be reused
        return (composition_space < original_space * 0.7 and 
                len(atomic_refs) >= 2 and
                any(self.part_usage.get(ref, 0) > 0 for ref in atomic_refs))
    
    def _collect_composite_patterns(self, data) -> None:
        """Collect all composite patterns and count their usage."""
        def collect_from_value(value):
            if isinstance(value, str) and ' ' in value and value.count('#') >= 2:
                # This looks like a composite pattern
                self.composite_patterns[value] += 1
            elif isinstance(value, dict):
                for v in value.values():
                    collect_from_value(v)
            elif isinstance(value, list):
                for item in value:
                    collect_from_value(item)
        
        for key, value in data.items():
            if not key.startswith('#'):  # Only check actual elements, not references
                collect_from_value(value)
    
    def _create_macros(self, min_usage: int = 2) -> None:
        """Create macro references for frequently used composite patterns."""
        for pattern, count in self.composite_patterns.items():
            if count >= min_usage:
                # Create a macro reference
                macro_ref = f"#M{len(self.macro_refs) + 1}"
                self.macro_refs[pattern] = macro_ref
                self.reference_data[macro_ref] = pattern
                self.dedup_stats['macros_created'] += 1
                
                if self.debug_dedup:
                    print(f"MACRO CREATED: {macro_ref} for pattern '{pattern}' (used {count} times)")
            elif self.debug_dedup and count > 1:
                print(f"MACRO SKIPPED: pattern '{pattern}' only used {count} times (threshold: {min_usage})")
    
    def _apply_macros(self, data) -> None:
        """Replace composite patterns with macro references."""
        def replace_in_value(value):
            if isinstance(value, str) and value in self.macro_refs:
                self.dedup_stats['macro_usages'] += 1
                return self.macro_refs[value]
            elif isinstance(value, dict):
                return {k: replace_in_value(v) for k, v in value.items()}
            elif isinstance(value, list):
                return [replace_in_value(item) for item in value]
            return value
        
        # Replace in main data (but not in reference definitions)
        for key in list(data.keys()):
            if not key.startswith('#'):
                data[key] = replace_in_value(data[key])
    
    def _create_signature(self, data) -> str:
        """Create a signature for data to detect duplicates."""
        try:
            # Create a normalized JSON representation for comparison
            if isinstance(data, dict):
                # Sort keys for consistent comparison
                sorted_data = {k: data[k] for k in sorted(data.keys())}
                return f"dict:{json.dumps(sorted_data, sort_keys=True)}"
            elif isinstance(data, list):
                # Sort lists if they contain only strings (like children lists)
                if all(isinstance(item, str) for item in data):
                    sorted_data = sorted(data)
                    return f"list:{json.dumps(sorted_data)}"
                else:
                    return f"list:{json.dumps(data)}"
            else:
                return f"other:{json.dumps(data)}"
        except (TypeError, ValueError):
            # Fallback for non-serializable data
            return f"fallback:{str(data)}"
    
    def _get_global_attributes(self) -> Dict[str, Union[List[str], None]]:
        """Return common global attributes for TEI/XML."""
        return {
            # XML core attributes
            'xml:id': None,
            'xml:lang': None,
            'xml:base': None,
            'xml:space': ['default', 'preserve'],
            
            # Common TEI attributes
            'n': None,
            'rend': None,
            'rendition': None,
            'style': None,
            'cert': ['high', 'medium', 'low', 'unknown'],
            'resp': None,
            'source': None,
            'copyOf': None,
            'corresp': None,
            'synch': None,
            'sameAs': None,
            'exclude': None,
            'select': None,
            'next': None,
            'prev': None,
        }


def generate_autocomplete_map(
    relaxng_file: str,
    include_global_attrs: bool = True,
    sort_alphabetically: bool = True,
    deduplicate: bool = False,
    debug_dedup: bool = False
) -> Dict[str, Dict]:
    """
    Generate a CodeMirror autocomplete map from a RelaxNG schema file.
    
    Args:
        relaxng_file: Path to the RelaxNG schema file
        include_global_attrs: Whether to include common global attributes
        sort_alphabetically: Whether to sort children and attributes alphabetically
        deduplicate: Whether to deduplicate redundant data using references
        debug_dedup: Print debugging information about deduplication process
    
    Returns:
        Dictionary suitable for use as CodeMirror autocomplete map.
        If deduplicate=True, includes reference definitions with keys starting with '#'
        that should be resolved using the JavaScript function in the class documentation.
    
    Example:
        >>> autocomplete_map = generate_autocomplete_map('tei.rng')
        >>> print(autocomplete_map['title'])
        {'attrs': {'xml:lang': None, 'type': ['main', 'sub', 'desc']}}
        
        >>> # With deduplication
        >>> autocomplete_map = generate_autocomplete_map('tei.rng', deduplicate=True)
        >>> # May contain references like '#1' instead of duplicate data
    """
    parser = RelaxNGParser(include_global_attrs, sort_alphabetically, deduplicate, debug_dedup)
    return parser.parse_file(relaxng_file)


def save_autocomplete_map(
    relaxng_file: str,
    output_file: str,
    include_global_attrs: bool = True,
    sort_alphabetically: bool = True,
    deduplicate: bool = False,
    indent: int = 2
) -> None:
    """
    Generate and save a CodeMirror autocomplete map to a JSON file.
    
    Args:
        relaxng_file: Path to the RelaxNG schema file
        output_file: Path for the output JSON file
        include_global_attrs: Whether to include common global attributes
        sort_alphabetically: Whether to sort children and attributes alphabetically
        deduplicate: Whether to deduplicate redundant data using references
        indent: JSON indentation level
    """
    import json
    
    autocomplete_map = generate_autocomplete_map(
        relaxng_file, include_global_attrs, sort_alphabetically, deduplicate
    )
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(autocomplete_map, f, indent=indent, ensure_ascii=False)


if __name__ == '__main__':
    import sys
    import json
    
    if len(sys.argv) < 2:
        print("Usage: python relaxng_to_codemirror.py <relaxng_file> [output_file] [--deduplicate]")
        sys.exit(1)
    
    relaxng_file = sys.argv[1]
    output_file = None
    deduplicate = False
    
    # Parse arguments
    for i, arg in enumerate(sys.argv[2:], 2):
        if arg == '--deduplicate':
            deduplicate = True
        elif not output_file:
            output_file = arg
    
    if not output_file:
        output_file = 'autocomplete.json'
    
    try:
        print(f"Processing {relaxng_file}...")
        if deduplicate:
            print("Deduplication enabled")
        autocomplete_map = generate_autocomplete_map(relaxng_file, deduplicate=deduplicate)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(autocomplete_map, f, indent=2, ensure_ascii=False)
        
        print(f"Generated autocomplete map with {len(autocomplete_map)} elements")
        print(f"Saved to {output_file}")
        
    except Exception as e:
        print(f"Error: {e}")
        sys.exit(1)